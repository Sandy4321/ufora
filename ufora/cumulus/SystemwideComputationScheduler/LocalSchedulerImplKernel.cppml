/***************************************************************************
   Copyright 2015 Ufora Inc.

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
****************************************************************************/
#include "LocalSchedulerImplKernel.hppml"
#include "../../FORA/VectorDataManager/VectorDataManager.hppml"
#include "../../FORA/TypedFora/ABI/BigVectorLayouts.hppml"
#include "../../core/threading/TimedLock.hpp"
#include "../../core/threading/CallbackScheduler.hppml"
#include "../../core/threading/CallbackSchedulerFactory.hppml"
#include "../../core/StringUtil.hpp"
#include "../../core/Memory.hpp"
#include "../../core/threading/Queue.hpp"
#include "../../core/threading/LimitToTime.hpp"
#include "../AddDropFinalState.hppml"
#include "../PageLayoutCalculator/NeighborhoodsOnMachines.hppml"
#include <iomanip>

using namespace Cumulus::PageLayoutCalculator;

namespace Cumulus {
namespace SystemwideComputationScheduler {

const double kTimeBetweenPageResets = 5.0;
const double kMaxLoadToFillACpu = .5;
const double kBroadcastInterval = .1;

LocalSchedulerImplKernel::LocalSchedulerImplKernel(
			uint64_t vdmMaxPageSizeInBytes,
			uint64_t vdmMemoryLimitInBytes,
        	MachineId inOwnMachineId,
			long inActiveThreadCount,
			boost::function1<void, InitiateComputationMove> onInitiateComputationMoved,
			boost::function1<void, CumulusComponentMessageCreated> onCumulusComponentMessageCreated
			) : 
		mOwnMachineId(inOwnMachineId),
		mOnInitiateComputationMoved(onInitiateComputationMoved),
		mOnCumulusComponentMessageCreated(onCumulusComponentMessageCreated),
		mActiveThreadCount(inActiveThreadCount),
		mComputationPageDependencies(
			boost::bind(&LocalSchedulerImplKernel::checkComputationsBlockedOnNonexistantPages_, this, boost::arg<1>())
			),
		mRandomGenerator(inOwnMachineId.guid()[0] + inOwnMachineId.guid()[1]),
		mCliqueRequestGuid(inOwnMachineId.guid() + hash_type(1)),
		mBigVectorLayouts(new TypedFora::Abi::BigVectorLayouts()),
		mComputationBehaviorPredictor(
			mBigVectorLayouts
			),
		mInitializationParameters(
			vdmMaxPageSizeInBytes,
			vdmMemoryLimitInBytes,
			inOwnMachineId,
			inActiveThreadCount
			),
		mComputationsMoved(0),
		mLastDumpTime(0),
		mMachineLoads(inOwnMachineId, inActiveThreadCount),
		mTimesCalledSinceLastDump(0),
		mLastBroadcast(0),
		mLastBroadcastLoad(0),
		mLocalToGlobalMessageThrottler(
			boost::bind(&LocalSchedulerImplKernel::sendLocalToGlobalSchedulerMessage, this, boost::arg<1>())
			),
		mTotalMoveAttempts(0),
		mTotalSplitAttempts(0),
		mCalculationsCompleted(0)
	{
	mCurrentMachines.insert(mOwnMachineId);
	mComputationPageDependencies.addMachine(mOwnMachineId);
	mSystemwidePageRefcountTracker.reset(
		new SystemwidePageRefcountTracker(
			mBigVectorLayouts,
			CallbackScheduler::singletonForTesting(),
			boost::function1<void, SystemwidePageRefcountTrackerEvent>()
			)
		);
	mSystemwidePageRefcountTracker->addMachine(mOwnMachineId);
	}

void LocalSchedulerImplKernel::sendSchedulerToComputationMessage(SchedulerToComputationMessage msg)
	{
	mOnCumulusComponentMessageCreated(
		CumulusComponentMessageCreated(
			CumulusComponentMessage::SchedulerToComputation(msg),
			CumulusComponentEndpointSet::SpecificWorker(msg.computationMachine()),
			CumulusComponentType::ActiveComputations()
			)
		);
	}

void LocalSchedulerImplKernel::sendLocalToGlobalSchedulerMessage(LocalToGlobalSchedulerMessage msg)
	{
	mOnCumulusComponentMessageCreated(
		CumulusComponentMessageCreated(
			CumulusComponentMessage::LocalToGlobalScheduler(msg),
			CumulusComponentEndpointSet::LeaderMachine(),
			CumulusComponentType::GlobalScheduler()
			)
		);
	}

Nullable<VectorDataID> LocalSchedulerImplKernel::pageToVectorDataId_(Fora::PageId inPage)
	{
	if (inPage.isInternal())
		return null() << VectorDataID::canonical(inPage);

	auto it = mPageToVectorDataIdMap.find(inPage);

	if (it != mPageToVectorDataIdMap.end())
		return null() << it->second;

	return null();
	}

void LocalSchedulerImplKernel::consumePageEvent(
								const Fora::PageRefcountEvent& inEvent, 
								Cumulus::MachineId onMachineId
								)
	{
	mSystemwidePageRefcountTracker->consumePageEvent(inEvent, onMachineId);

	@match Fora::PageRefcountEvent(inEvent)
		-| BigVectorReferenced(newLayout) ->> {
			mBigVectorLayouts->registerNewLayout(newLayout);

			mBigVectorRefcounts[newLayout.identity()]++;

			if (mBigVectorRefcounts[newLayout.identity()] == 1)
				mBigVectorLayoutMap[newLayout.identity()] = newLayout;
			}
		-| BigVectorNoLongerReferenced(identity) ->> {
			mBigVectorRefcounts[identity]--;

			if (mBigVectorRefcounts[identity] == 0)
				{
				mBigVectorRefcounts.erase(identity);
				mBigVectorLayoutMap.erase(identity);
				}
			}
		-| PageAddedToRam(page) ->> {
			mBytesAddedByMachine[onMachineId] += page.bytecount();

			mComputationPageDependencies.handlePageRefcountEvent(inEvent, onMachineId);
			}
		-| PageDroppedFromRam(page) ->> {
			mComputationPageDependencies.handlePageRefcountEvent(inEvent, onMachineId);
			}
		-| PageMarkedNotLoadable(page) ->> {
			mComputationPageDependencies.pageDroppedAcrossEntireSystem(page);
			}
		-| PageAddedToDisk(page) ->> {
			mBytesSentToDiskByMachine[onMachineId] += page.bytecount();
			}
		-| _ ->> {
			}
	}

void LocalSchedulerImplKernel::pageNoLongerReferencedAcrossSystem(Fora::PageId page)
	{
	LOG_DEBUG << "On " << prettyPrintString(mOwnMachineId) 
		<< ", page " << prettyPrintString(page) << " no longer referenced across system.";

	mComputationPageDependencies.pageDroppedAcrossEntireSystem(page);
	}

bool LocalSchedulerImplKernel::hasPageBeenDroppedAcrossEntireSystem_(Fora::PageId page)
	{
	return mComputationPageDependencies.hasPageBeenDroppedAcrossEntireSystem(page) || 
			mSystemwidePageRefcountTracker->isPageNotLoadable(page);
	}

void LocalSchedulerImplKernel::addMachine(MachineId inMachine)
	{
	mCurrentMachines.insert(inMachine);

	mComputationPageDependencies.addMachine(inMachine);

	mSystemwidePageRefcountTracker->addMachine(inMachine);

	//make sure there's a processor load entry
	mMachineLoads.addMachine(inMachine);
	}

void LocalSchedulerImplKernel::computationComputeStatusChanged(
											const ComputationComputeStatusChanged& change
											)
	{
	if (change.isComputingNow())
		mCurrentlyComputingComputations.insert(change.computation());
	else
		mCurrentlyComputingComputations.erase(change.computation());
	}

void LocalSchedulerImplKernel::computationStatusChanged(
								const LocalComputationPriorityAndStatusChanged& change,
								double curTime
								)
	{
	mComputationPageDependencies.dropComputation(change.computation());

	mComputationBehaviorPredictor.computationStatusChanged(change);

	@match LocalComputationPriorityAndStatusChanged(change)
		-| Active(computation, priority, status, statistics) ->> {

			if (status.isFinished() && !mLocalComputationStatuses[change.computation()].isFinished())
				mCalculationsCompleted++;

			mLocalComputationStatuses[change.computation()] = status;
			mLocalComputationPriorities[change.computation()] = priority;

			if (priority.isCircular())
				sendSchedulerToComputationMessage(
					SchedulerToComputationMessage::MarkSelfCircular(
						computation,
						hash_type(),
						mOwnMachineId,
						mOwnMachineId
						)
					);

			if (status.isBlockedOnVectorLoad())
				mComputationsBlockedOnVectorsLocally.insert(change.computation());
			else
				mComputationsBlockedOnVectorsLocally.erase(change.computation());

			if (status.isBlockedOnComputations())
				mComputationsBlockedOnComputationsLocally.insert(change.computation());
			else
				mComputationsBlockedOnComputationsLocally.erase(change.computation());

			if (status.isFinished() && computation.isSplit())
				mSplitComputationsFinishedButUncollected.insert(computation);
			else
				mSplitComputationsFinishedButUncollected.erase(computation);

			@match ComputationStatus(status)
				-| BlockedOnComputations(computations) ->> {
					}
				-| Finished() ->> {
					mCurrentlyComputingComputationsLastPageResetTimes.erase(computation);
					mComputationPageDependencies.dropComputation(computation);
					mTryingToSplit.erase(computation);
					}
				-| BlockedOnVectorLoad(slice) ->> {
					bool blocked = false;

					for (auto v: convertBigVectorSliceToVDIDs_(slice))
						if (v.isExternal())
							{
							auto it = mPageToVectorDataIdMap.find(v.getPage());

							if (it == mPageToVectorDataIdMap.end())
								{
								mPageToVectorDataIdMap[v.getPage()] = v;
								if (!mSystemwidePageRefcountTracker->isPageAnywhereInRam(v.getPage())
										&& !mSystemwidePageRefcountTracker->isPageAnywhereOnDisk(v.getPage()))
									blocked = true;
								}
							}

					if (blocked)
						sendLocalToGlobalSchedulerMessage(
							LocalToGlobalSchedulerMessage::TriggerPageLayoutRecompute()
							);

					handleComputationBlocked_(computation, slice);
					}
				-| _ ->> {}

			if (!priority.isNull() && status.isComputable())
				computationIsComputable_(computation, priority);
			else
				computationNotComputable_(computation);
			}
		-| Inactive(computation) ->> {
			mLocalComputationStatuses.erase(computation);
			mLocalComputationPriorities.erase(computation);
			mComputationsBlockedOnVectorsLocally.erase(computation);
			mComputationsBlockedOnComputationsLocally.erase(computation);
			computationNotComputable_(computation);
			mTryingToSplit.erase(computation);
			mSplitComputationsFinishedButUncollected.erase(computation);
			}
	
	updatePageLoadForComputation_(change.computation(), curTime);

	if (mComputationPageDependencies.isDeferred(change.computation()))
		{
		ImmutableTreeSet<Fora::PageId> currentActiveGroup = 
			filterOutPagesDroppedAcrossSystem_(
				mComputationBehaviorPredictor.getActivePageLoadGroup(change.computation()).pages()
				);

		mComputationPageDependencies.updateDeferredComputation(
			change.computation(), 
			currentActiveGroup
			);
		}
	}


void LocalSchedulerImplKernel::computationIsComputable_(ComputationId computation, ComputationPriority newPriority)
	{
	if (!mCurrentlyComputableComputations.hasKey(computation) || 
			mCurrentlyComputableComputations.getValue(computation).second != 
				newPriority)
		{
		mCurrentlyComputableComputationsByPriority.set(computation, newPriority);
		mCurrentlyComputableComputations.set(
			computation, 
			make_pair(0, newPriority)
			);
		}
	}

void LocalSchedulerImplKernel::computationNotComputable_(ComputationId computation)
	{
	mMachineLoads.computationNotComputable(computation);

	if (mCurrentlyComputableComputations.hasKey(computation))
		{
		mCurrentlyComputableComputations.drop(computation);
		mCurrentlyComputableComputationsByPriority.drop(computation);
		}
	}

Neighborhood LocalSchedulerImplKernel::filterOutPagesDroppedAcrossSystem_(Neighborhood n)
	{
	ImmutableTreeSet<Fora::PageId> pageList = filterOutPagesDroppedAcrossSystem_(n.pages());

	if (pageList.size() == n.pages().size())
		return n;

	return Neighborhood(pageList);
	}

ImmutableTreeSet<Fora::PageId> 
		LocalSchedulerImplKernel::filterOutPagesDroppedAcrossSystem_(
						ImmutableTreeSet<Fora::PageId> pages
						)
	{
	ImmutableTreeSet<Fora::PageId> result;
	bool anyRemoved = false;

	for (long ix = 0; ix < pages.size(); ix++)
		{
		Fora::PageId p = pages[ix];

		if (hasPageBeenDroppedAcrossEntireSystem_(p))
			{
			if (!anyRemoved)
				{
				anyRemoved = true;
				result = pages.slice(0,ix);
				}
			}
		else
			{
			if (anyRemoved)
				result = result + p;
			}
		}

	if (!anyRemoved)
		return pages;
	else
		return result;
	}

ImmutableTreeMap<Fora::PageId, double> 
		LocalSchedulerImplKernel::filterOutPagesDroppedAcrossSystem_(
						ImmutableTreeMap<Fora::PageId, double> pages
						)
	{
	ImmutableTreeMap<Fora::PageId, double> result;

	for (auto page: pages)
		{
		if (!hasPageBeenDroppedAcrossEntireSystem_(page.first))
			result = result + page;
		}

	return result;
	}

void LocalSchedulerImplKernel::checkComputationsBlockedOnNonexistantPages_(ComputationId computation)
	{
	if (mLocalComputationStatuses.find(computation) == mLocalComputationStatuses.end())
		return;

	@match ComputationStatus(mLocalComputationStatuses[computation])
		-| BlockedOnVectorLoad(slice) ->> {
			handleComputationBlocked_(computation, slice);
			}
		-| _ ->> {
			}
	}

ImmutableTreeSet<VectorDataID> LocalSchedulerImplKernel::convertBigVectorSliceToVDIDs_(
																Fora::BigVectorSlice slice
																)
	{
	ImmutableTreeVector<TypedFora::Abi::VectorDataIDSlice> slices = 
		mBigVectorLayouts->getLayoutForId(slice.identity())
			.slicesCoveringRange(slice.indexLow(), slice.indexHigh());

	ImmutableTreeSet<VectorDataID> result;

	for (auto s: slices)
		{
		Fora::PageId page = s.vector().getPage();

		if (page != s.vector().getPage() && pageToVectorDataId_(page))
			result = result + *pageToVectorDataId_(page);
		else
			result = result + s.vector();
		}

	return result;
	}

ComputationLoad LocalSchedulerImplKernel::calculateCurrentLoadForComputation(ComputationId computation)
	{
	auto statusIt = mLocalComputationStatuses.find(computation);

	if (statusIt == mLocalComputationStatuses.end())
		return ComputationLoad();

	if (!statusIt->second.isComputable() && !statusIt->second.isBlockedOnVectorLoad())
		return ComputationLoad();

	if (mLocalComputationPriorities[computation].isNull())
		return ComputationLoad();

	pair<Neighborhood, double> load;

	load = mComputationBehaviorPredictor.getActivePageLoadPrediction(computation);
	load.first = filterOutPagesDroppedAcrossSystem_(load.first);

	bool isRepresentedAnywhere = 
		mComputationPageDependencies.machineHoldingAllPages(load.first.pages()).isValue();

	return ComputationLoad(load.first, load.second, isRepresentedAnywhere);
	}

void LocalSchedulerImplKernel::updatePageLoadForComputation_(const ComputationId& id, double curTime)
	{
	ComputationLoad load = calculateCurrentLoadForComputation(id);

	if (load == ComputationLoad())
		{
		if (mComputationLoadsBroadcast.find(id) != mComputationLoadsBroadcast.end())
			{
			mLocalToGlobalMessageThrottler.send(
				LocalToGlobalSchedulerMessage::ErasePageLoad(id, mOwnMachineId),
				curTime
				);
			mComputationLoadsBroadcast.erase(id);
			}
		}
	else
		{
		if (mComputationLoadsBroadcast.find(id) == mComputationLoadsBroadcast.end() || 
				mComputationLoadsBroadcast[id] != load)
			{
			mLocalToGlobalMessageThrottler.send(
				LocalToGlobalSchedulerMessage::SetPageLoad(
					id,
					mOwnMachineId,
					load.pages().pages(),
					load.timeElapsed(),
					!load.isRepresentedAnywhere()
					),
				curTime
				);
			mComputationLoadsBroadcast[id] = load;
			}
		}
	}

void LocalSchedulerImplKernel::handleComputationBlocked_(
						const ComputationId& computation, 
						const Fora::BigVectorSlice& slice
						)
	{
	LOG_INFO << computation.guid() << " is blocked on " << slice;

	if (mMachineLoads.computationsMoving().hasKey(computation))
		{
		LOG_INFO << "not worrying about " << computation.guid()
			<< " because it's already being moved.";
		return;
		}

	ImmutableTreeSet<VectorDataID> vdidsBlockedOn = convertBigVectorSliceToVDIDs_(slice);

	if (mCurrentlyComputableComputations.hasKey(computation))
		{
		mCurrentlyComputableComputations.drop(computation);
		mCurrentlyComputableComputationsByPriority.drop(computation);
		}

	ImmutableTreeSet<Fora::PageId> currentActiveGroup = 
		filterOutPagesDroppedAcrossSystem_(
			mComputationBehaviorPredictor.getActivePageLoadGroup(computation).pages()
			);

	for (auto vdid: vdidsBlockedOn)
		currentActiveGroup = currentActiveGroup + vdid.getPage();

	mComputationPageDependencies.deferComputation(computation, currentActiveGroup);

	//if all the vectors are together in one place we can move to that machine and load
	ImmutableTreeSet<Cumulus::MachineId> machinesWithPagesLoaded = 
					mComputationPageDependencies.machinesHoldingAllPages(currentActiveGroup);

	Nullable<Cumulus::MachineId> newMachine = 
			pickMachineWithLeastLoad_(machinesWithPagesLoaded);

	if (!newMachine)
		{
		LOG_INFO << "Can't activate computation " << computation.guid()
			<< " depending on " << prettyPrintStringWithoutWrapping(currentActiveGroup)
			<< " since they're not loaded all on one machine. Deferring."
			;

		if (currentActiveGroup.size() == 1 && 
					mComputationPageDependencies.computationsDependingOn(
												currentActiveGroup[0]
												).size() == 1 &&
					!currentActiveGroup[0].isInternal() && 
					wantsToSplit_()
					)
			//attempt to split this computation
			sendSchedulerToComputationMessage(
				SchedulerToComputationMessage::Split(
					computation, 
					hash_type(), 
					mOwnMachineId,
					mOwnMachineId,
					0.0
					)
				);

		return;
		}
	
	if (*newMachine == mOwnMachineId)
		return;
	
	if (mMachineLoads.computationsMoving().hasKey(computation))
		return;

	mMachineLoads.taskIsMoving(computation, *newMachine);

	auto identity = mBigVectorLayouts->getLayoutForId(slice.identity()).identity();

	LOG_INFO << "moving blocked computation " << computation.guid() << " to " 
			<< prettyPrintStringWithoutWrapping(*newMachine) 
			<< ". moving=" << mMachineLoads.computationsMoving().size()
			<< ". slice=["
			<< slice.indexLow() << ","
			<< slice.indexHigh() << "]. "
			<< ". identity=" << prettyPrintStringWithoutWrapping(identity)
			<< ". CAG of " << prettyPrintStringWithoutWrapping(
				filterOutPagesDroppedAcrossSystem_(currentActiveGroup)
				)
			;

	logTryingToMove_(computation, *newMachine);

	mComputationsMoved++;

	mOnInitiateComputationMoved(
		InitiateComputationMove(
			computation,
			*newMachine
			)
		);
	}

Nullable<MachineId> LocalSchedulerImplKernel::pickMachineWithLeastLoad_(
												ImmutableTreeSet<Cumulus::MachineId> machines
												)
	{
	if (machines.size() == 0)
		return null();

	return null() << Ufora::math::Random::pickRandomlyFromSet(machines, mRandomGenerator);
	}

void LocalSchedulerImplKernel::logTryingToMove_(
												const ComputationId& computation,
												const MachineId& machine
												)
	{
	mComputationsMoved++;

	LOG_DEBUG << "moving " << prettyPrintString(computation) << " from " 
		<< prettyPrintString(mOwnMachineId)
		<< " to " 
		<< prettyPrintString(machine)
		;
	}

void LocalSchedulerImplKernel::handleCumulusComponentMessage(
	            CumulusComponentMessage message, 
    	        CumulusClientOrMachine source, 
        	    CumulusComponentType componentType,
        	    double curTime
        	    )
	{
	@match CumulusComponentMessage(message)
		-| ActiveComputationsToLocalScheduler(
				InitiateComputationMoveResponse(computation, success)
				) ->> {
			handleInitiateComputationMoveResponse(computation, success);
			}
		-| ActiveComputationsToLocalScheduler(
				ComputationToScheduler(msg)
				) ->> {
			handleComputationToSchedulerMessage(msg, curTime);
			}
		-| LocalToLocalSchedulerBroadcast(msg) ->> {
			handleLocalToLocalSchedulerBroadcastMessage(msg);
			}
		-| LocalComputationPriorityAndStatus(msg) ->> {
			computationStatusChanged(msg, curTime);
			}
		-| ComputationComputeStatus(msg) ->> {
			computationComputeStatusChanged(msg);
			}
	}

void LocalSchedulerImplKernel::handleInitiateComputationMoveResponse(
							const ComputationId& inComputation,
							bool isSuccess
							)
	{
	mMachineLoads.taskIsNotMoving(inComputation);

	if (!isSuccess)
		checkComputationsBlockedOnNonexistantPages_(inComputation);
	}

void LocalSchedulerImplKernel::handleComputationToSchedulerMessage(
							const ComputationToSchedulerMessage& response,
							double curTime
							)
	{
	if (response.isEvents())
		{
		mComputationBehaviorPredictor.handleComputationEvents(
			response.computation(), 
			response.getEvents().events()
			);

		for (auto load: mComputationBehaviorPredictor.extractAndResetObservedLoads())
			mLocalToGlobalMessageThrottler.send(
				LocalToGlobalSchedulerMessage::RecordObservedLoad(load.first.pages(), load.second),
				curTime
				);

		return;
		}

	if (response.guid() == hash_type())
		mTryingToSplit.erase(response.computation());

	if (response.isCliques())
		{
		auto cliques = response.getCliques().cliques();

		long already = 0;
		long fresh = 0;

		for (auto c: cliques)
			{
			PageLayoutCalculator::Neighborhood n(c);

			if (mCliquesObservedAlready.find(n) != mCliquesObservedAlready.end())
				already++;
			else
				{
				fresh++;

				mLocalToGlobalMessageThrottler.send(
					LocalToGlobalSchedulerMessage::CliqueObserved(c),
					curTime
					);

				mCliquesObservedAlready.insert(n);
				}
			}

		LOG_INFO << mOwnMachineId << " observed " << cliques.size() 
			<< " cliques from " << response.computation().guid() << ", of which " 
			<< fresh << " were fresh. Total=" << mCliquesObservedAlready.size()
			;

		if (cliques.size() > 10 && fresh > 5 && 
				mComputationsBlockedOnVectorsLocally.size() < mActiveThreadCount)
			{
			sendSchedulerToComputationMessage(
				SchedulerToComputationMessage::Split(
					response.computation(), 
					hash_type(), 
					mOwnMachineId,
					mOwnMachineId,
					0.0
					)
				);
			}
		}

	if (mBlockedAndWaitingForCliques.hasValue(response.guid()))
		mBlockedAndWaitingForCliques.dropValue(response.guid());
	}

Nullable<ComputationId> LocalSchedulerImplKernel::searchForSplittableComputation_()
	{
	if (!mCurrentlyComputableComputations.size())
		return null();

	bool hasRecycled = false;

	while (true)
		{
		for (auto it = mCurrentlyComputableComputations.getValueToKeys().rbegin(); 
					it != mCurrentlyComputableComputations.getValueToKeys().rend();
					++it
					)
			{
			if (it->first.first != 0)
				//we've tried to split everything in the non-recycled list. At this point,
				//all entries will have '-1' in the first part of the pair, indicating that
				//we tried them once. We should just wait for the responses to come back.
				break;

			for (auto it2 = it->second.begin(); it2 != it->second.end(); ++it2)
				{
				if (mTryingToSplit.find(*it2) == mTryingToSplit.end())
					return null() << *it2;
				}
			}

		if (!hasRecycled)
			{
			recyclePriorityList_();
			hasRecycled = true;
			}
		else
			return null();
		}

	return null();
	}

void LocalSchedulerImplKernel::recyclePriorityList_()
	{
	//reset all pairs for which the first element is less than zero.
	while (mCurrentlyComputableComputations.lowestValue().first < 0)
		{
		ComputationId id = 
			*mCurrentlyComputableComputations.getValueToKeys().begin()->second.begin();

		pair<long, ComputationPriority> priorityPair = 
			mCurrentlyComputableComputations.getValue(id);

		priorityPair.first = 0;

		mCurrentlyComputableComputations.set(id, priorityPair);
		}
	}

bool LocalSchedulerImplKernel::wantsToSplit_()
	{
	if (mComputationsBlockedOnVectorsLocally.size() > mActiveThreadCount * 2)
		return false;

	if (mMachineLoads.wantsDisableSplitDueToSystemCapacityOverload())
		return false;

	return mMachineLoads.computationsMoving().size() + 
			mCurrentlyComputableComputations.size() +
			mTryingToSplit.size() < 
				std::max<long>(mActiveThreadCount + 4, mActiveThreadCount * 2);
	}

void LocalSchedulerImplKernel::splitOrMoveIfNeeded(double curTime)
	{
	mLocalToGlobalMessageThrottler.sendIfNecessary(curTime);

	resetPageDataIfNecessary_(curTime);

	if (wantsToSplit_())
		tryToSplitSomething_();

	//this is not a very good way to determine whether we should issue 'SearchForCliques',
	//because it doesn't track how many computations are blocked or active on a per machine basis
	if (mCurrentlyComputableComputations.size() + mBlockedAndWaitingForCliques.size() < 
				mActiveThreadCount &&
			mBlockedAndWaitingForCliques.size() < mComputationsBlockedOnVectorsLocally.size())
		while (mBlockedAndWaitingForCliques.size() < mComputationsBlockedOnVectorsLocally.size())
			if (!tryToCalculateCliquesForSomething_())
				break;
	
	updateCurrentProcessorLoadAndBroadcast_(curTime);

	double t0 = curClock();
	long passes = 0;
	while (mMachineLoads.shouldTryToMoveSomething())
		{
		passes++;
		if (!tryToMoveSomething_())
			break;
		}
	double t1 = curClock();
	mTimesCalledSinceLastDump++;

	if (curClock() > mLastDumpTime + 2.0)
		{
		LOGGER_INFO_T log = LOGGER_INFO;

		map<MachineId, long> recv;
		for (const auto& mAB: mMostRecentBroadcasts)
			for (auto machineAndCount: mAB.second.moveTargets())
				recv[machineAndCount.first] += machineAndCount.second;
		

		for (auto machineAndLoad: mMachineLoads.getMachineLoads().getKeyToValue())
			{
			if (machineAndLoad.first == mOwnMachineId)
				log << "** ";
			else
				log << "   ";

			log << machineAndLoad.first << " -> " 
				<< std::setw(6)
				<< machineAndLoad.second << ". receiving " 
				<< std::setw(6)
				<< recv[machineAndLoad.first]
				<< ". "
				;

			log << "comp: " << std::setw(6) << mMostRecentBroadcasts[machineAndLoad.first].computableComputations() << ". ";
			log << "uncomp: " << std::setw(6) << mMostRecentBroadcasts[machineAndLoad.first].noncomputableComputations() << ". ";
			log << "mov: " << std::setw(6) << mMostRecentBroadcasts[machineAndLoad.first].movingComputations() << ". ";
			log << "splt: " << std::setw(6) << mMostRecentBroadcasts[machineAndLoad.first].splittingComputations() << ". ";
			log << "moveLag: " << std::setprecision(2) << std::setw(6) << mMostRecentBroadcasts[machineAndLoad.first].ageOfOldestMove() << ". ";
			log << "totMov: " << std::setw(6) << mMostRecentBroadcasts[machineAndLoad.first].totalMoves() << ". ";
			log << "totSplt: " << std::setw(6) << mMostRecentBroadcasts[machineAndLoad.first].totalSplits() << ". ";
			log << "everFin: " << std::setw(6) << mMostRecentBroadcasts[machineAndLoad.first].totalCompleted() << ". ";
			log << "vecload: " << std::setw(6) << mMostRecentBroadcasts[machineAndLoad.first].blockedComputations() << ". ";
			log << "onSubcomp: " << std::setw(6) << mMostRecentBroadcasts[machineAndLoad.first].blockedOnSubcomputations() << ". ";
			log << "finished: " << std::setw(6) << mMostRecentBroadcasts[machineAndLoad.first].finishedButUncollected() << ". ";
			log << "wantsMove: " << (mMachineLoads.shouldMoveToMachine(machineAndLoad.first) ?"yes":"no") << ". ";

			log << "\n";
			}

		long moveable = 0;
		for (auto compAndPri: mCurrentlyComputableComputationsByPriority.getKeyToValue())
			{
			auto comp = compAndPri.first;
			if (mCurrentlyComputableComputations.hasKey(comp) && 
						mTryingToSplit.find(comp) == mTryingToSplit.end() && 
										!mMachineLoads.computationsMoving().hasKey(comp))
				{
				ImmutableTreeSet<MachineId> possibleMoves = 
					mComputationPageDependencies.machinesHoldingAllPages(
						filterOutPagesDroppedAcrossSystem_(
							mComputationBehaviorPredictor.getActivePageLoadGroup(comp).pages()
							)
						);
				if (possibleMoves.size())
					moveable++;
				}
			}

		log << "moveable: " << moveable << "\n";

		log << "passes: " << passes << " over " << t1 - t0 << "\n";
		log << "Called " << mTimesCalledSinceLastDump << " since last dump.\n";
		mTimesCalledSinceLastDump = 0;

		mLastDumpTime = curClock();
		}
	}

bool LocalSchedulerImplKernel::tryToCalculateCliquesForSomething_()
	{
	while (true)
		{
		for (auto id: mComputationsBlockedOnVectorsLocally)
			if (mRecentlyCalculatedForCliques.find(id) == 
					mRecentlyCalculatedForCliques.end() && 
					!mBlockedAndWaitingForCliques.hasKey(id))
				{
				if (mBlockedAndWaitingForCliques.size() < mActiveThreadCount)
					{
					hash_type guid = mCliqueRequestGuid;
					mCliqueRequestGuid = mCliqueRequestGuid + hash_type(1);

					mBlockedAndWaitingForCliques.set(id, guid);
					mRecentlyCalculatedForCliques.insert(id);

					LOG_INFO << "Requesting cliques for " << id.guid() << " on " << mOwnMachineId
						<< ". Total computing cliques: " << mBlockedAndWaitingForCliques.size() 
						<< "/" << mComputationsBlockedOnVectorsLocally.size();

					sendSchedulerToComputationMessage(
						SchedulerToComputationMessage::SearchForCliques(
							id,
							guid,
							mOwnMachineId,
							mOwnMachineId,
							1.0
							)
						);

					return true;
					}
				}

		if (!mRecentlyCalculatedForCliques.size())
			return false;

		//if we get here, reset the list and try again
		mRecentlyCalculatedForCliques.clear();
		}
	}

void LocalSchedulerImplKernel::resetPageDataIfNecessary_(double curTime)
	{
	for (auto computation: mCurrentlyComputingComputations)
		{
		if (mCurrentlyComputingComputationsLastPageResetTimes[computation] < 
													curTime - kTimeBetweenPageResets)
			{
			sendSchedulerToComputationMessage(
				SchedulerToComputationMessage::ResetPageDataAndBroadcast(
					computation, 
					hash_type(1),
					mOwnMachineId,
					mOwnMachineId
					)
				);

			mCurrentlyComputingComputationsLastPageResetTimes[computation] = curTime;
			}
		}
	}

void LocalSchedulerImplKernel::handleLocalToLocalSchedulerBroadcastMessage(
										LocalToLocalSchedulerBroadcastMessage message
										)
	{
	@match LocalToLocalSchedulerBroadcastMessage(message)
		-| CurrentLoad() with (sourceMachine) ->> {
			mMachineLoads.setLoad(
				sourceMachine, 
				message.getCurrentLoad().expectedCpusWorthOfCompute(),
				message.getCurrentLoad().blockedComputations()
				);

			mMostRecentBroadcasts[sourceMachine] = message.getCurrentLoad();
			}
	}

void LocalSchedulerImplKernel::updateCurrentProcessorLoadAndBroadcast_(double curTime)
	{
	double total = mCurrentlyComputableComputations.size();

	long newLoadLevel = total;

	if (newLoadLevel != mMachineLoads.ownLoadRaw())
		mMachineLoads.setLoad(mOwnMachineId, newLoadLevel, mComputationsBlockedOnVectorsLocally.size());

	if (curTime - mLastBroadcast > kBroadcastInterval || 
			newLoadLevel > mActiveThreadCount && mLastBroadcastLoad < mActiveThreadCount ||
			newLoadLevel < mActiveThreadCount && mLastBroadcastLoad > mActiveThreadCount
			)
		{
		mLastBroadcast = curTime;
		mLastBroadcastLoad = newLoadLevel;

		ImmutableTreeMap<MachineId, long> sending;
		for (const auto& machineAndComps: mMachineLoads.computationsMoving().getValueToKeys())
			sending = sending + machineAndComps.first + (long)machineAndComps.second.size();

		LocalToLocalSchedulerBroadcastMessage msg = 
			LocalToLocalSchedulerBroadcastMessage::CurrentLoad(
				mOwnMachineId,
				mCurrentlyComputableComputations.size(), 
				mLocalComputationStatuses.size() - mCurrentlyComputableComputations.size(),
				newLoadLevel,
				mMachineLoads.computationsMoving().size(),
				mTryingToSplit.size(),
				mMachineLoads.ageOfOldestMove(),
				sending,
				mComputationsMoved,
				mTotalSplitAttempts,
				mCalculationsCompleted,
				mComputationsBlockedOnVectorsLocally.size(),
				mComputationsBlockedOnComputationsLocally.size(),
				mSplitComputationsFinishedButUncollected.size()
				);

		mOnCumulusComponentMessageCreated(
			CumulusComponentMessageCreated(
				CumulusComponentMessage::LocalToLocalSchedulerBroadcast(
					msg
					),
				CumulusComponentEndpointSet::AllWorkersExceptSelf(),
				CumulusComponentType::LocalScheduler()
				)
			);

		mMostRecentBroadcasts[mOwnMachineId] = msg.getCurrentLoad();
		}
	}

bool LocalSchedulerImplKernel::tryToMoveSomething_()
	{
	double t0 = curClock();
	if (mMoveCandidates.size() == 0)
		{
		for (auto compAndPri: mCurrentlyComputableComputationsByPriority.getKeyToValue())
			mMoveCandidates.push_back(compAndPri.first);

		for (long k = 0; k + 1 < mMoveCandidates.size(); k++)
			std::swap(
				mMoveCandidates[k], 
				mMoveCandidates[k + (mMoveCandidates.size() - k) * mRandomGenerator()]
				);
		}

	while (mMoveCandidates.size())
		{
		ComputationId comp = mMoveCandidates.back();
		mMoveCandidates.pop_back();

		if (mCurrentlyComputableComputations.hasKey(comp) && 
					mTryingToSplit.find(comp) == mTryingToSplit.end() && 
									!mMachineLoads.computationsMoving().hasKey(comp))
			{
			ImmutableTreeSet<MachineId> possibleMoves = 
				mComputationPageDependencies.machinesHoldingAllPages(
					filterOutPagesDroppedAcrossSystem_(
						mComputationBehaviorPredictor.getActivePageLoadGroup(comp).pages()
						)
					);

			if (possibleMoves.size())
				{
				std::vector<pair<hash_type, MachineId> > hashRing;
				for (auto machine: possibleMoves)
					hashRing.push_back(make_pair(machine.guid() + mOwnMachineId.guid(), machine));
				std::sort(hashRing.begin(), hashRing.end());

				std::vector<MachineId> possible;

				for (long k = 0; k < hashRing.size() && k < 5; k++)
					possible.push_back(hashRing[k].second);

				for (long k = 0; k + 1 < possible.size(); k++)
					std::swap(
						possible[k], 
						possible[k + (possible.size() - k) * mRandomGenerator()]
						);

				for (auto machine: possible)
					if (mMachineLoads.shouldMoveToMachine(machine))
						{
						logTryingToMove_(comp, machine);

						mMachineLoads.taskIsMoving(comp, machine);

						mComputationsMoved++;

						mOnInitiateComputationMoved(
							InitiateComputationMove(comp, machine)
							);

						return true;
						}
				}
			}
		}

	if (curClock() - t0 > .2)
		LOG_WARN << "Spent " << curClock() - t0 << " trying to split";

	return false;
	}

void LocalSchedulerImplKernel::tryToSplitSomething_()
	{
	Nullable<ComputationId> toSplit = searchForSplittableComputation_();

	if (toSplit)
		tryToSplitComputation_(*toSplit);
	}

void LocalSchedulerImplKernel::tryToSplitComputation_(ComputationId toSplit)
	{
	if (mTryingToSplit.find(toSplit) != mTryingToSplit.end())
		return;

	mTryingToSplit.insert(toSplit);

	const static double kMinTimeToComputeBeforeSplitting = .1;

	mTotalSplitAttempts++;

	sendSchedulerToComputationMessage(
		SchedulerToComputationMessage::Split(
			toSplit, 
			hash_type(), 
			mOwnMachineId,
			mOwnMachineId,
			kMinTimeToComputeBeforeSplitting
			)
		);

	if (mCurrentlyComputableComputations.hasKey(toSplit))
		{
		pair<long, ComputationPriority> curPriorityPair = 
			mCurrentlyComputableComputations.getValue(toSplit);

		lassert(curPriorityPair.first == 0);

		curPriorityPair.first = -1;

		mCurrentlyComputableComputations.set(toSplit, curPriorityPair);
		}
	}

}
}

