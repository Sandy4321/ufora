/***************************************************************************
    Copyright 2015 Ufora Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at
 
       http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
****************************************************************************/
#pragma once

#include "ThreadGroup.hppml"
#include "ThreadGroupState.hppml"
#include "../ComputationEvent.hppml"
#include "../LocalToLocalSchedulerMessage.hppml"
#include "../LocalToGlobalSchedulerMessage.hppml"
#include "../DistributedDataTasks/MachineHashTable.hppml"

namespace Cumulus {

namespace SystemwideComputationScheduler {

class ThreadGroupStatusTracker {
public:
    ThreadGroupStatusTracker(
                boost::function1<void, LocalToLocalSchedulerMessage> inOnSendLocal,
                boost::function1<void, LocalToGlobalSchedulerMessage> inOnSendGlobal,
                MachineHashTable& inMachineTable,
                MachineId ownMachine,
                int64_t maxGroupSize
                ) :
            mOnSendLocal(inOnSendLocal),
            mOnSendGlobal(inOnSendGlobal),
            mMachineTable(inMachineTable),
            mOwnMachine(ownMachine),
            mMaxGroupSize(maxGroupSize)
        {
        }

    void handleComputationEvents(ComputationId id, ImmutableTreeVector<ComputationEvent> events)
        {
        ThreadGroup group = ThreadGroup::groupFor(id);

        auto targetMachine = machineFor(group);

        if (targetMachine == mOwnMachine)
            handleComputationEventsLocally(group, events);
        else
            mOnSendLocal(
                LocalToLocalSchedulerMessage::ThreadGroupEvents(
                    mOwnMachine,
                    targetMachine,
                    group,
                    events
                    )
                );
        }

    void handleLocalToLocalSchedulerMessage(const LocalToLocalSchedulerMessage& in)
        {
        @match LocalToLocalSchedulerMessage(in)
            -| ThreadGroupStatusChanged(change) ->> {
                handleStatusChangedLocally(change);
                }
            -| ThreadGroupChildChanged(update) ->> {
                handleChildGroupChanged(update);
                }
            -| ThreadGroupEvents(group, events) ->> {
                handleComputationEventsLocally(group, events);
                }
            -| ThreadGroupActivatedByParent(group) ->> {
                activateThreadGroup(group);
                }
        }

    void handleComputationEventsLocally(ThreadGroup group, ImmutableTreeVector<ComputationEvent> events)
        {
        ensureThreadGroup(group);

        mStates[group]->handleEvents(events, mPagesDropped);
        groupChanged(group);
        }

    //the status of this computation changed, and we are the owning machine for this threadgroup.
    void handleStatusChangedLocally(const LocalComputationPriorityAndStatusChanged& change)
        {
        lassert(machineFor(change.computation()) == mOwnMachine);

        @match LocalComputationPriorityAndStatusChanged(change)
            -| Active(c, _, newStatus, stats) ->> {
                ThreadGroup group = ThreadGroup::groupFor(c);

                if (mFinishedGroups.find(group) != mFinishedGroups.end())
                    return;

                ensureThreadGroup(group);

                mStates[group]->handleUpdateToSelf(newStatus, stats, mPagesDropped);
                groupChanged(group);
                }
            -| _ ->> {}
        }

    void groupChanged(ThreadGroup group)
        {
        auto it = mStates.find(group);

        if (it == mStates.end())
            return;

        if (mStates[group]->isFinished() || mStates[group]->wantsBroadcast())
            broadcastGroup(group);

        if (mStates[group]->isFinished())
            {
            mFinishedGroups.insert(group);
            mStates.erase(group);
            mPagesUsedByThreadGroups.dropKey(group);
            }
        else
            {
            for (auto child: mStates[group]->extractChildrenToActivate(mMaxGroupSize))
                {
                if (machineFor(child) == mOwnMachine)
                    activateThreadGroup(child);
                else
                    mOnSendLocal(
                        LocalToLocalSchedulerMessage::ThreadGroupActivatedByParent(
                            mOwnMachine,
                            machineFor(child),
                            child
                            )
                        );
                }
            }
        }

    void broadcastGroup(ThreadGroup group)
        {
        std::set<Fora::PageId> newPagesSinceLastBroadcast;
        std::set<Fora::PageId> droppedPagesSinceLastBroadcast;

        mStates[group]->extractPageChangesAndMarkBroadcast(
                            newPagesSinceLastBroadcast, 
                            droppedPagesSinceLastBroadcast
                            );

        ThreadGroupStateUpdate update(
            group, 
            ImmutableTreeVector<Fora::PageId>(newPagesSinceLastBroadcast), 
            ImmutableTreeVector<Fora::PageId>(droppedPagesSinceLastBroadcast), 
            mStates[group]->totalChildBytes(),
            mStates[group]->totalTimeElapsed(),
            mStates[group]->isFinished(),
            mStates[group]->isActive()
            );

        //first, send locally
        auto nParent = group.parent();
        if (nParent)
            {
            auto parent = *nParent;

            MachineId targetMachine = machineFor(parent);

            if (targetMachine == mOwnMachine)
                handleChildGroupChanged(update);
            else
                mOnSendLocal(
                    LocalToLocalSchedulerMessage::ThreadGroupChildChanged(
                        mOwnMachine,
                        targetMachine,
                        update
                        )
                    );
            }

        if (!update.isActive() && mGroupsSentToGlobalScheduler.find(group) != mGroupsSentToGlobalScheduler.end() || 
                update.isActive())
            {
            if (mGroupsSentToGlobalScheduler.find(group) != mGroupsSentToGlobalScheduler.end())
                //this is the first time we're sending the update
                {
                update.droppedPages() = emptyTreeVec();
                update.newPages() = mStates[group]->getAllPages();
                }

            mOnSendGlobal(LocalToGlobalSchedulerMessage::ThreadGroupChanged(update));

            if (update.isActive() && !update.isFinished())
                mGroupsSentToGlobalScheduler.insert(group);
            else
                mGroupsSentToGlobalScheduler.erase(group);
            }
        }

    void ensureThreadGroup(ThreadGroup group)
        {
        if (mStates.find(group) == mStates.end())
            {
            mStates[group].reset(new ThreadGroupState(group, mPagesUsedByThreadGroups));
            if (group.isRoot())
                mStates[group]->markActive();
            }
        }

    void activateThreadGroup(ThreadGroup group)
        {
        ensureThreadGroup(group);
        mStates[group]->markActive();
        groupChanged(group);
        }


    void handleChildGroupChanged(ThreadGroupStateUpdate update)
        {
        ThreadGroup parent = *update.group().parent();

        lassert(machineFor(parent) == mOwnMachine);

        if (mFinishedGroups.find(parent) != mFinishedGroups.end())
            return;

        ensureThreadGroup(parent);

        mStates[parent]->handleChildGroupChanged(update, mPagesDropped);
        groupChanged(parent);
        }

    void pageNoLongerReferencedAcrossSystem(Fora::PageId page)
        {
        mPagesDropped.insert(page);
        for (auto group: mPagesUsedByThreadGroups.getKeys(page))
            if (mStates.find(group) != mStates.end())
                mStates[group]->pageNoLongerReferencedAcrossSystem(page);

        mPagesUsedByThreadGroups.dropValue(page);
        }

    //the status of the computation changed on this machine. If the owning machine
    //for this ThreadGroup is local, handle it here. Otherwise send it to another box.
    void handleStatusChanged(const LocalComputationPriorityAndStatusChanged& change)
        {
        ComputationId id = change.computation();

        MachineId targetMachine = machineFor(id);

        if (targetMachine == mOwnMachine)
            handleStatusChangedLocally(change);
        else
            mOnSendLocal(
                LocalToLocalSchedulerMessage::ThreadGroupStatusChanged(
                    mOwnMachine,
                    targetMachine,
                    change
                    )
                );
        }

    MachineId machineFor(const ComputationId& computationId)
        {
        return mMachineTable.lookup(ThreadGroup::groupFor(computationId).hash());
        }

    MachineId machineFor(const ThreadGroup& group)
        {
        return mMachineTable.lookup(group.hash());
        }

private:
    boost::function1<void, LocalToLocalSchedulerMessage> mOnSendLocal;

    boost::function1<void, LocalToGlobalSchedulerMessage> mOnSendGlobal;

    int64_t mMaxGroupSize;
    
    MachineId mOwnMachine;

    MachineHashTable& mMachineTable;

    TwoWaySetMap<ThreadGroup, Fora::PageId> mPagesUsedByThreadGroups;

    std::set<Fora::PageId> mPagesDropped;

    std::set<ThreadGroup> mGroupsSentToGlobalScheduler;

    std::map<ThreadGroup, boost::shared_ptr<ThreadGroupState> > mStates;

    std::set<ThreadGroup> mFinishedGroups;
};

}

}